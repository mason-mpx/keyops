package auth

import (
	"context"
	"fmt"
	"log"
	"net"
	"net/http"
	"os/exec"
	"runtime"
	"strconv"
	"sync"
	"time"

	"github.com/fisker/zjump-backend/internal/model"
	"github.com/fisker/zjump-backend/internal/repository"
	"github.com/fisker/zjump-backend/pkg/distributed"
	pkgredis "github.com/fisker/zjump-backend/pkg/redis"
)

// HostMonitorService ä¸»æœºçŠ¶æ€ç›‘æ§æœåŠ¡
type HostMonitorService struct {
	hostRepo      *repository.HostRepository
	settingRepo   *repository.SettingRepository
	interval      time.Duration
	stopChan      chan struct{}
	wg            sync.WaitGroup
	config        *model.HostMonitorConfig
	configMu      sync.RWMutex
	ticker        *time.Ticker
	tickerMu      sync.Mutex    // ä¿æŠ¤ ticker çš„å¹¶å‘è®¿é—®
	isRunning     bool          // å®šæ—¶å™¨æ˜¯å¦æ­£åœ¨è¿è¡Œ
	tickerStop    chan struct{} // ç”¨äºåœæ­¢å½“å‰è¿è¡Œçš„ ticker goroutine
	tickerStopped chan struct{} // ç”¨äºç¡®è®¤ ticker goroutine å·²åœæ­¢
}

// NewHostMonitorService åˆ›å»ºä¸»æœºç›‘æ§æœåŠ¡
func NewHostMonitorService(hostRepo *repository.HostRepository, settingRepo *repository.SettingRepository, intervalMinutes int) *HostMonitorService {
	if intervalMinutes <= 0 {
		intervalMinutes = 5 // é»˜è®¤5åˆ†é’Ÿ
	}

	service := &HostMonitorService{
		hostRepo:    hostRepo,
		settingRepo: settingRepo,
		interval:    time.Duration(intervalMinutes) * time.Minute,
		stopChan:    make(chan struct{}),
		config: &model.HostMonitorConfig{
			Enabled:    true,
			Interval:   intervalMinutes,
			Method:     model.MonitorMethodTCP,
			Timeout:    3,
			Concurrent: 20,
		},
	}

	// ä»æ•°æ®åº“åŠ è½½é…ç½®
	service.loadConfig()

	// å¦‚æœå¯ç”¨äº† Redisï¼Œç›‘å¬é…ç½®å˜æ›´
	if pkgredis.IsEnabled() {
		configSync := distributed.NewConfigSyncManager(pkgredis.GetClient(), "zjump:config:changes")
		configSync.AddListener(func(key string, value string) {
			// åªå¤„ç†ä¸»æœºç›‘æ§ç›¸å…³çš„é…ç½®å˜æ›´
			if key == "host_monitor_enabled" ||
				key == "host_monitor_interval" ||
				key == "host_monitor_method" ||
				key == "host_monitor_timeout" ||
				key == "host_monitor_concurrent" {
				log.Printf("[HostMonitor] Received config change from Redis: %s = %s", key, value)
				service.ReloadConfig()
			}
		})
		go configSync.Start()
	}

	return service
}

// loadConfig ä»æ•°æ®åº“åŠ è½½é…ç½®
func (s *HostMonitorService) loadConfig() {
	s.configMu.Lock()
	defer s.configMu.Unlock()

	// è¯»å–é…ç½®é¡¹
	if enabled, err := s.settingRepo.Get("host_monitor_enabled"); err == nil && enabled != "" {
		s.config.Enabled = enabled == "true"
	}
	if interval, err := s.settingRepo.Get("host_monitor_interval"); err == nil && interval != "" {
		if val, err := strconv.Atoi(interval); err == nil && val > 0 {
			s.config.Interval = val
			s.interval = time.Duration(val) * time.Minute
		}
	}
	if method, err := s.settingRepo.Get("host_monitor_method"); err == nil && method != "" {
		s.config.Method = method
	}
	if timeout, err := s.settingRepo.Get("host_monitor_timeout"); err == nil && timeout != "" {
		if val, err := strconv.Atoi(timeout); err == nil && val > 0 {
			s.config.Timeout = val
		}
	}
	if concurrent, err := s.settingRepo.Get("host_monitor_concurrent"); err == nil && concurrent != "" {
		if val, err := strconv.Atoi(concurrent); err == nil && val > 0 {
			s.config.Concurrent = val
		}
	}

	log.Printf("[HostMonitor] Config loaded: enabled=%v, interval=%dm, method=%s, timeout=%ds, concurrent=%d",
		s.config.Enabled, s.config.Interval, s.config.Method, s.config.Timeout, s.config.Concurrent)
}

// ReloadConfig é‡æ–°åŠ è½½é…ç½®
func (s *HostMonitorService) ReloadConfig() {
	oldInterval := s.interval

	s.configMu.RLock()
	oldEnabled := s.config.Enabled
	s.configMu.RUnlock()

	s.loadConfig()

	s.configMu.RLock()
	newEnabled := s.config.Enabled
	s.configMu.RUnlock()

	// æ£€æŸ¥å¯ç”¨çŠ¶æ€æ˜¯å¦å˜åŒ–
	if oldEnabled != newEnabled {
		if newEnabled {
			log.Printf("[HostMonitor]  Monitoring enabled, starting ticker...")
			s.startTicker()
		} else {
			log.Printf("[HostMonitor] â¸ï¸  Monitoring disabled, stopping ticker...")
			s.stopTicker()
		}
		return
	}

	// å¦‚æœå¯ç”¨çŠ¶æ€æœªå˜ï¼Œä½†é—´éš”æ—¶é—´æ”¹å˜äº†ï¼Œé‡å¯å®šæ—¶å™¨
	if newEnabled && oldInterval != s.interval {
		log.Printf("[HostMonitor] Interval changed from %v to %v, restarting ticker", oldInterval, s.interval)
		s.stopTicker()
		s.startTicker()
	}
}

// GetConfig è·å–å½“å‰é…ç½®
func (s *HostMonitorService) GetConfig() model.HostMonitorConfig {
	s.configMu.RLock()
	defer s.configMu.RUnlock()
	return *s.config
}

// Start å¯åŠ¨ç›‘æ§æœåŠ¡ï¼ˆå®šæ—¶æ£€æŸ¥ï¼Œå¯åŠ¨æ—¶ä¸ç«‹å³æ‰§è¡Œï¼‰
func (s *HostMonitorService) Start() {
	s.configMu.RLock()
	enabled := s.config.Enabled
	s.configMu.RUnlock()

	if enabled {
		log.Printf("[HostMonitor]  Host monitoring service started (interval: %v)", s.interval)
		s.startTicker()
	} else {
		log.Printf("[HostMonitor] â¸ï¸  Host monitoring is disabled, ticker not started")
	}
}

// startTicker å¯åŠ¨å®šæ—¶å™¨
func (s *HostMonitorService) startTicker() {
	s.tickerMu.Lock()

	// å¦‚æœå·²ç»åœ¨è¿è¡Œï¼Œå…ˆå®Œå…¨åœæ­¢æ—§çš„
	if s.isRunning {
		s.tickerMu.Unlock()
		s.stopTickerInternal() // å®Œå…¨åœæ­¢æ—§çš„ goroutine
		s.tickerMu.Lock()
	}

	log.Printf("[HostMonitor] â–¶ï¸  Starting ticker (interval: %v)", s.interval)

	// åˆ›å»ºæ–°çš„åœæ­¢ä¿¡å· channel
	s.tickerStop = make(chan struct{})
	s.tickerStopped = make(chan struct{})
	s.ticker = time.NewTicker(s.interval)
	s.isRunning = true

	// ä¿å­˜ channels çš„å¼•ç”¨ï¼Œé¿å…åœ¨ goroutine ä¸­è¢«æ›¿æ¢
	tickerStop := s.tickerStop
	tickerStopped := s.tickerStopped
	ticker := s.ticker

	s.tickerMu.Unlock()

	s.wg.Add(1)
	go func() {
		defer func() {
			s.wg.Done()
			close(tickerStopped) // é€šçŸ¥å·²å®Œå…¨åœæ­¢
		}()

		for {
			select {
			case <-ticker.C:
				s.checkAllHosts()

			case <-tickerStop:
				// æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ¸…ç†å¹¶é€€å‡º
				log.Println("[HostMonitor] â¹ï¸  Ticker goroutine stopping...")
				ticker.Stop()
				return

			case <-s.stopChan:
				// æ•´ä¸ªæœåŠ¡åœæ­¢
				log.Println("[HostMonitor]  Host monitoring service stopped")
				ticker.Stop()
				s.tickerMu.Lock()
				s.isRunning = false
				s.ticker = nil
				s.tickerMu.Unlock()
				return
			}
		}
	}()
}

// stopTickerInternal å†…éƒ¨æ–¹æ³•ï¼šå®Œå…¨åœæ­¢å½“å‰çš„ ticker goroutine
func (s *HostMonitorService) stopTickerInternal() {
	s.tickerMu.Lock()

	if !s.isRunning {
		s.tickerMu.Unlock()
		return
	}

	log.Printf("[HostMonitor] â¹ï¸  Stopping ticker...")

	tickerStop := s.tickerStop
	tickerStopped := s.tickerStopped
	s.isRunning = false

	s.tickerMu.Unlock()

	// å‘é€åœæ­¢ä¿¡å·
	close(tickerStop)

	// ç­‰å¾… goroutine å®Œå…¨åœæ­¢
	<-tickerStopped

	s.tickerMu.Lock()
	s.ticker = nil
	s.tickerStop = nil
	s.tickerStopped = nil
	s.tickerMu.Unlock()

	log.Printf("[HostMonitor]  Ticker stopped successfully")
}

// stopTicker åœæ­¢å®šæ—¶å™¨ï¼ˆå…¬å¼€æ–¹æ³•ï¼‰
func (s *HostMonitorService) stopTicker() {
	s.stopTickerInternal()
}

// Stop åœæ­¢ç›‘æ§æœåŠ¡
func (s *HostMonitorService) Stop() {
	close(s.stopChan)
	s.wg.Wait()
}

// CheckAllHosts æ£€æŸ¥æ‰€æœ‰ä¸»æœºçŠ¶æ€ï¼ˆå…¬å¼€æ–¹æ³•ï¼‰
func (s *HostMonitorService) CheckAllHosts() {
	s.checkAllHosts()
}

// checkAllHosts æ£€æŸ¥æ‰€æœ‰ä¸»æœºçŠ¶æ€ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰
func (s *HostMonitorService) checkAllHosts() {
	// æ£€æŸ¥æ˜¯å¦å¯ç”¨ç›‘æ§
	s.configMu.RLock()
	enabled := s.config.Enabled
	s.configMu.RUnlock()

	if !enabled {
		log.Printf("[HostMonitor] â¸ï¸  Monitoring is disabled, skipping check...")
		return
	}

	// å¦‚æœå¯ç”¨äº† Redisï¼Œä½¿ç”¨åˆ†å¸ƒå¼é”
	if pkgredis.IsEnabled() {
		s.checkAllHostsWithLock()
	} else {
		s.doCheckAllHosts()
	}
}

// checkAllHostsWithLock ä½¿ç”¨åˆ†å¸ƒå¼é”æ£€æŸ¥æ‰€æœ‰ä¸»æœº
// å¦‚æœRedisæœªå¯ç”¨ï¼Œä¼šé™çº§ä¸ºå•æœºæ¨¡å¼ï¼ˆé”è·å–å¤±è´¥ä½†ä¸å½±å“ä¸»æµç¨‹ï¼‰
func (s *HostMonitorService) checkAllHostsWithLock() {
	// åˆ›å»ºåˆ†å¸ƒå¼é”ï¼Œé”çš„æœ‰æ•ˆæœŸä¸ºæ£€æµ‹é—´éš”çš„2å€ï¼ˆé˜²æ­¢æ£€æµ‹æ—¶é—´è¿‡é•¿ï¼‰
	lockKey := "zjump:host_monitor:lock"
	lock := distributed.NewRedisLock(pkgredis.GetClient(), lockKey, s.interval*2)

	// å°è¯•è·å–é”
	acquired, err := lock.TryLock()
	if err != nil {
		log.Printf("[HostMonitor]  Failed to acquire lock: %v", err)
		return
	}

	if !acquired {
		log.Printf("[HostMonitor] â­ï¸  Another instance is checking hosts, skipping...")
		return
	}

	defer func() {
		if err := lock.Unlock(); err != nil {
			log.Printf("[HostMonitor]   Failed to release lock: %v", err)
		}
	}()

	log.Printf("[HostMonitor] ğŸ”’ Acquired distributed lock, starting check...")
	s.doCheckAllHosts()
}

// doCheckAllHosts æ‰§è¡Œå®é™…çš„ä¸»æœºæ£€æµ‹
func (s *HostMonitorService) doCheckAllHosts() {
	s.configMu.RLock()
	method := s.config.Method
	concurrent := s.config.Concurrent
	s.configMu.RUnlock()

	log.Printf("[HostMonitor]  Starting host status check (method: %s)...", method)
	startTime := time.Now()

	// è·å–æ‰€æœ‰ä¸»æœºï¼ˆä¸åˆ†é¡µï¼Œç›´æ¥è·å–å…¨éƒ¨ï¼‰
	hosts, _, err := s.hostRepo.FindAllWithPagination(1, 10000, "", []string{})
	if err != nil {
		log.Printf("[HostMonitor]  Failed to load hosts: %v", err)
		return
	}

	if len(hosts) == 0 {
		log.Println("[HostMonitor] No hosts to monitor")
		return
	}

	log.Printf("[HostMonitor] Checking %d hosts (concurrent: %d)...", len(hosts), concurrent)

	// ä½¿ç”¨goroutineå¹¶å‘æ£€æŸ¥ï¼Œä½†é™åˆ¶å¹¶å‘æ•°
	sem := make(chan struct{}, concurrent)
	var wg sync.WaitGroup

	onlineCount := 0
	offlineCount := 0
	var mu sync.Mutex

	for i := range hosts {
		wg.Add(1)
		go func(host *model.Host) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			sem <- struct{}{}
			defer func() { <-sem }()

			// æ£€æŸ¥ä¸»æœºçŠ¶æ€
			online := s.checkHostStatus(host)

			// æ›´æ–°çŠ¶æ€
			newStatus := "offline"
			if online {
				newStatus = "online"
			}

			// åªæœ‰çŠ¶æ€å˜åŒ–æ—¶æ‰æ›´æ–°æ•°æ®åº“
			if host.Status != newStatus {
				oldStatus := host.Status
				if err := s.hostRepo.UpdateStatus(host.ID, newStatus); err != nil {
					log.Printf("[HostMonitor] Failed to update status for %s (%s): %v",
						host.Name, host.IP, err)
				} else {
					log.Printf("[HostMonitor]  Host %s (%s): %s â†’ %s",
						host.Name, host.IP, oldStatus, newStatus)
				}
			}

			mu.Lock()
			if online {
				onlineCount++
			} else {
				offlineCount++
			}
			mu.Unlock()
		}(&hosts[i])
	}

	wg.Wait()

	duration := time.Since(startTime)
	log.Printf("[HostMonitor]  Check completed in %v: %d online, %d offline (total: %d)",
		duration, onlineCount, offlineCount, len(hosts))
}

// checkHostStatus æ£€æŸ¥å•ä¸ªä¸»æœºçŠ¶æ€
func (s *HostMonitorService) checkHostStatus(host *model.Host) bool {
	s.configMu.RLock()
	method := s.config.Method
	timeout := time.Duration(s.config.Timeout) * time.Second
	s.configMu.RUnlock()

	// æ ¹æ®é…ç½®çš„æ£€æµ‹æ–¹å¼è¿›è¡Œæ£€æµ‹
	switch method {
	case model.MonitorMethodICMP:
		return s.checkICMP(host.IP, timeout)
	case model.MonitorMethodHTTP:
		return s.checkHTTP(host.IP, host.Port, timeout)
	case model.MonitorMethodTCP:
		fallthrough
	default:
		// ä½¿ç”¨ä¸»æœºé…ç½®çš„ç«¯å£è¿›è¡ŒTCPæ£€æµ‹
		port := host.Port
		if port == 0 {
			port = 22 // é»˜è®¤SSHç«¯å£
		}
		return s.checkTCPPort(host.IP, port, timeout)
	}
}

// checkTCPPort æ£€æŸ¥TCPç«¯å£æ˜¯å¦å¯è¾¾
func (s *HostMonitorService) checkTCPPort(ip string, port int, timeout time.Duration) bool {
	address := fmt.Sprintf("%s:%d", ip, port)
	conn, err := net.DialTimeout("tcp", address, timeout)
	if err != nil {
		return false
	}
	conn.Close()
	return true
}

// checkICMP é€šè¿‡ICMP Pingæ£€æŸ¥ä¸»æœºæ˜¯å¦åœ¨çº¿
func (s *HostMonitorService) checkICMP(ip string, timeout time.Duration) bool {
	var cmd *exec.Cmd

	// æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©ä¸åŒçš„pingå‘½ä»¤
	switch runtime.GOOS {
	case "windows":
		// Windows: ping -n 1 -w <timeout_ms> <ip>
		timeoutMs := int(timeout.Milliseconds())
		cmd = exec.Command("ping", "-n", "1", "-w", fmt.Sprintf("%d", timeoutMs), ip)
	case "darwin":
		// macOS: ping -c 1 -W <timeout_ms> <ip>
		timeoutMs := int(timeout.Milliseconds())
		cmd = exec.Command("ping", "-c", "1", "-W", fmt.Sprintf("%d", timeoutMs), ip)
	default:
		// Linux: ping -c 1 -W <timeout_sec> <ip>
		timeoutSec := int(timeout.Seconds())
		if timeoutSec < 1 {
			timeoutSec = 1
		}
		cmd = exec.Command("ping", "-c", "1", "-W", fmt.Sprintf("%d", timeoutSec), ip)
	}

	ctx, cancel := context.WithTimeout(context.Background(), timeout+time.Second)
	defer cancel()

	cmd = exec.CommandContext(ctx, cmd.Path, cmd.Args[1:]...)
	err := cmd.Run()
	return err == nil
}

// checkHTTP é€šè¿‡HTTPè¯·æ±‚æ£€æŸ¥ä¸»æœºæ˜¯å¦åœ¨çº¿
func (s *HostMonitorService) checkHTTP(ip string, port int, timeout time.Duration) bool {
	// é»˜è®¤ä½¿ç”¨80ç«¯å£
	if port == 22 || port == 0 {
		port = 80
	}

	url := fmt.Sprintf("http://%s:%d", ip, port)

	client := &http.Client{
		Timeout: timeout,
		CheckRedirect: func(req *http.Request, via []*http.Request) error {
			return http.ErrUseLastResponse // ä¸è·Ÿéšé‡å®šå‘
		},
	}

	resp, err := client.Get(url)
	if err != nil {
		return false
	}
	defer resp.Body.Close()

	// ä»»ä½•HTTPå“åº”éƒ½ç®—åœ¨çº¿ï¼ˆåŒ…æ‹¬4xxã€5xxé”™è¯¯ï¼‰
	return true
}

// CheckHostStatusNow ç«‹å³æ£€æŸ¥æŒ‡å®šä¸»æœºçŠ¶æ€ï¼ˆæ‰‹åŠ¨è§¦å‘ï¼‰
func (s *HostMonitorService) CheckHostStatusNow(hostID string) (bool, error) {
	host, err := s.hostRepo.FindByID(hostID)
	if err != nil {
		return false, fmt.Errorf("ä¸»æœºä¸å­˜åœ¨: %w", err)
	}

	online := s.checkHostStatus(host)

	newStatus := "offline"
	if online {
		newStatus = "online"
	}

	if err := s.hostRepo.UpdateStatus(host.ID, newStatus); err != nil {
		return online, fmt.Errorf("æ›´æ–°çŠ¶æ€å¤±è´¥: %w", err)
	}

	log.Printf("[HostMonitor] Manual check: %s (%s) is %s", host.Name, host.IP, newStatus)
	return online, nil
}
